<!DOCTYPE html>
<html lang="pt-br">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>LABIT</title>
  <link rel="stylesheet" href="style.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css">

</head>
<body>

  <!-- NAVBAR -->
  <nav class="navbar">
    <div class="nav-logo">
      <img src="labit.jpg" alt="LABIT">
    </div>
    <ul class="nav-links">
      <li><i class="bi bi-house-door"></i> INÍCIO</li>
      <li><i class="bi bi-people"></i> SOBRE NÓS</li>
      <li><i class="bi bi-journal-text"></i> ARTIGOS</li>
      <li><i class="bi bi-search"></i> PROJETOS </li>
      <li><i class="bi bi-newspaper"></i> NOTÍCIAS</li>
      <li><i class="bi bi-envelope"></i> CONTATO</li>
    </ul>
    <div class="nav-search">
    <i class="bi bi-search"></i>
    <input type="text" placeholder="Buscar...">
  </div>
  </nav>

  <!-- CARD COM CONTEÚDO -->
  <div class="card">
    <section class="breadcrumb">
      ARTIGOS / SEGURANÇA DA INFORMAÇÃO / PROMPT INJECTION
    </section>

    <h2>Fichamento do artigo: “Prompt Injection Attack Against LLM-integrated Applications”</h2>

    <div class="meta">
    <span><i class="bi bi-calendar-event"></i> 16 de maio de 2025</span>
    <span><i class="bi bi-at"></i>labit.ufpa</span>
  </div>


    <p>
      O artigo <strong>“Prompt Injection Attack Against LLM-integrated Applications”</strong> foi desenvolvido pelos pesquisadores Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Zihao Wang, Xiaofeng Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng e Yang Liu, e foi publicado na revista "Cryptography and Security" da Universidade de Cornell em 2024.
    </p>
    <p>
      O principal objetivo do artigo foi analisar os impactos que os ataques de injeção imediata causam em aplicações reais que utilizam LLMs. Isso é feito por meio de uma nova técnica proposta pelos autores, chamada HOUEEU, dividida em três partes: um prompt pré-construído, um contexto indutor de prompt injection e uma carga maliciosa.
    </p>
    <p>
      Para chegar aos resultados, os autores testaram a técnica HOUEEU em 36 aplicações reais de mercado que utilizam LLMs.
    </p>
    <p>
      Como conclusão, os autores destacaram que sua técnica permitiu realizar ataques graves a aplicações que até então eram consideradas seguras. Das 36 aplicações testadas, 31 foram vulneráveis, demonstrando os riscos e a necessidade de medidas de mitigação.
    </p>
    <p>
      A pesquisa desenvolvida pode ter um impacto direto no cenário de segurança, ao evidenciar vulnerabilidades reais e propor critérios para entender e combater ataques de prompt injection em LLMs.
    </p>
  </div>
  <footer class="footer">
  <p>© 2025 LABIT. Todos os direitos reservados.</p>
</footer>

</body>

</html>
