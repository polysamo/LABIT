<!DOCTYPE html>
<html lang="pt-br">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>LABIT</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <header class="banner">
    <img src="labit.jpg" alt="Logo" class="logo">
    <img src="ufpa_logo.png" alt="Logo" class="logo">
    <h1>Fichamento <span> LABIT</span></h1>
  </header>

  <section class="breadcrumb">
    ACADÊMICO / ARTIGOS  / SEGURANÇA DA INFORMAÇÃO / PROMPT INJECTION
  </section>

  <main class="content">
    <h2>
      Análise e Fichamento do artigo: “Prompt Injection Attack Against LLM-integrated Applications”
    </h2>
    <div class="meta">
      <span>15 de maio de 2025</span>
      <span>@labit.ufpa </span>
      <span>Deixe um comentário</span>
    </div>

    <p>
      O artigo <strong>“Prompt Injection Attack Against LLM-integrated Applications
”</strong>
       foi desenvolvido pelos pesquisadores Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Zihao Wang, Xiaofeng Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng e Yang Liu e foi publicado na "Cryptography and Security"da Universidade de Cornell em 2024.
    </p>
    <p>
        O principal objetivo do artigo foi analisar os impactos que os ataques de injeção imediata causam em aplicações reais que utilizam as LLMs. Então isso é realizado, basicamente, por meio da proposta que os autores do artigo apresentaram, que é uma nova técnica de injeção de prompt, chamada HOUEEU. Ela é dividida em três partes: um prompt pré-construído, um contexto indutor de prompt injection e uma mensagem/carga maliciosa. Assim, realizando os devidos ataques. 
    </p>
        Para chegar aos resultados, os autores propuseram a utilização de uma nova técnica de injeção de prompt, chamada HOUEEU. Ela é dividida em três partes: um prompt pré-construído, um contexto indutor de prompt injection e uma mensagem/carga maliciosa. Assim, realizando os devidos ataques. 
    </p>
    <p>
        Nesse artigo, os autores utilizaram as seguintes tecnologias e métricas de avaliação: os autores utilizaram a proposta deles, que foi a técnica chamada HOUEEU para realizar os ataques de prompt injection e testaram em 36 aplicações reais que usam o LLM e utilizadas no mercado. 
    </p>
    <P>
        Como conclusão dos estudos, os autores destacaram que por meio da nova técnica realizaram e fizeram ataques graves a essas aplicações e que até então eram desconhecidas, como o uso arbitrário irrestrito de LLM e roubo de prompt de aplicativo descomplicado. Além disso, das 36 aplicações, 31 eram suscetíveis à injeção imediata. Logo, esse estudo apresenta os possíveis riscos de ataques de injeção imediata quanto às possíveis táticas de mitigação.
    </P>
    <P>
      Fazendo uma relação, a pesquisa desenvolvida por *autores(são muitos)* pode ter um impacto direto no cenário de prompt injection, já que esse artigo mostra como realizar esses ataques e as premissas do que ele pode ocasionar e as consequências desse ataque em aplicações reais integradas a LLMs
    </P>
  </main>

</body>
</html>
