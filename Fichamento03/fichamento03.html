<!DOCTYPE html>
<html lang="pt-br">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>LABIT</title>
  <link rel="stylesheet" href="style.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css">

</head>

<body>

  <!-- NAVBAR -->
  <nav class="navbar">
    <div class="nav-logo">
      <img src="labit.jpg" alt="LABIT">
    </div>
    <ul class="nav-links">
      <li><i class="bi bi-house-door"></i> INÍCIO</li>
      <li><i class="bi bi-people"></i> SOBRE NÓS</li>
      <li><i class="bi bi-journal-text"></i> ARTIGOS</li>
      <li><i class="bi bi-search"></i> PROJETOS </li>
      <li><i class="bi bi-newspaper"></i> NOTÍCIAS</li>
      <li><i class="bi bi-envelope"></i> CONTATO</li>
    </ul>
    <div class="nav-search">
      <i class="bi bi-search"></i>
      <input type="text" placeholder="Buscar...">
    </div>
  </nav>

  <!-- CARD COM CONTEÚDO -->
  <div class="card">
    <section class="breadcrumb">
      ARTIGOS / SEGURANÇA DA INFORMAÇÃO / PROMPT INJECTION
    </section>
    <div style="margin: 18px 0 30px 0;">
      <a href="../index.html" style="color:#446de1; font-weight:600; text-decoration:none; font-size:1.1em;"><i
          class="bi bi-arrow-left"></i> Voltar ao Início</a>
    </div>

    <h2>Fichamento do artigo: “Can Indirect Prompt Injection Attacks Be Detected and Removed?”</h2>

    <div class="meta">
      <span><i class="bi bi-calendar-event"></i> 05 de junho de 2025</span>
      <span><i class="bi bi-at"></i>labit.ufpa</span>
    </div>

    <hr class="divider">

    <p>
      O artigo <strong>“Can Indirect Prompt Injection Attacks Be Detected and Removed?”</strong> foi desenvolvido e
      escrito pelos pesquisadores Yulin Chen, Haoran Li, Yuan Sui, Yufei He, Yue Liu, Yangqiu Song, Bryan Hooi, além de
      ter sido publicado na "Cryptography and Security" da Universidade de Cornell em 2025 e disponibilizado como
      preprint no repositório arXiv em fevereiro de 2025 (arXiv:2502.16580v1 [cs.CR]).
    </p>
    <p>
      O estudo aborda a preocupação com os ataques indiretos de injeção de prompt, uma área que tem recebido menos
      atenção no cenário atual, onde a maioria dos estudos se concentra em defesas contra injeções diretas. Esses
      ataques indiretos envolvem a inserção de instruções maliciosas por meio de ferramentas externas, como motores de
      busca. Além disso, a pesquisa atual foca principalmente em métodos de detecção de injeção, negligenciando as
      estratégias de pós-processamento que visam mitigar os efeitos da injeção após sua detecção. Este artigo vai
      explorar a viabilidade de detectar e remover ataques de injeção indireta imediata, além de construir um conjunto
      de dados de referência para avaliação desses ataques.
    </p>
    <p>
      Os Grandes Modelos de Linguagem (LLMs) são grandes ferramentas, mas vulneráveis a ataques de injeção de prompt,
      que manipulam os modelos para executar instruções maliciosas, ignorando sua programação original. Existem dois
      tipos principais de ataques que podem ser amplamente categorizados: injeção direta de prompt (a), onde o atacante
      insere instruções diretamente no prompt do LLM, e injeção indireta de prompt (b), onde o atacante incorpora
      instruções maliciosas em fontes externas, como documentos da web, que são processados pelo LLM. Para combater
      esses ataques, as defesas exploradas incluem instruir os modelos a ignorar instruções injetadas, filtragem, que
      detecta e remove instruções maliciosas, e métodos de filtragem unificados, que combinam detecção e remoção para
      uma defesa mais abrangente.
    </p>
    <figure style="text-align:center; margin: 20px 0;">
      <img src="injeção direta e indireta.png" alt="indirect and direct injection" style="width:65%; height:auto;">
      <figcaption style="font-size: 0.9em; color: #555; margin-top: 8px;">
        Figura 1. Fonte: Artigo "Can Indirect Prompt Injection Attacks Be Detected and Removed?", 2025.
      </figcaption>
    </figure>
    <p>
      Essa pesquisa aborda essa lacuna, propondo um benchmark de avaliação para detectar e remover injeções indiretas de
      prompt. O estudo avalia o desempenho dos LLMs e modelos de detecção, cria dados de treinamento para detecção de
      injeções indiretas e investiga dois métodos de remoção: remoção por segmentação, que classifica e descarta
      segmentos contendo instruções injetadas, e remoção por extração, que treina modelos para identificar e remover o
      conteúdo injetado diretamente. Por fim, irá avaliar o desempenho da defesa contra injeções indiretas de prompt
      combinando métodos de detecção e remoção em uma abordagem unificada de filtragem.
    </p>
    </p><strong> Ataques e Defesas de Injeção de Prompt </strong></p>
    <p>
      Entre as estratégias de ataque mais comuns estão: o "prompt de ignorar", onde uma instrução maliciosa é anexada
      para manipular o modelo; a adição de respostas falsas, que engana o LLM fazendo-o acreditar que a entrada já foi
      processada, levando à execução de comandos indesejados; a combinação de múltiplas estratégias, que aumenta a
      eficácia do ataque; e a otimização de sufixos, que engana os modelos de maneira eficaz.
    </p>
    <p>
      Em resposta a essas ameaças, várias defesas de injeção de prompt foram propostas. Algumas dessas defesas incluem o
      uso de lembretes para reforçar o cumprimento das instruções originais, a utilização de tokens especiais para
      delimitar claramente a área de conteúdo dos dados, e o treinamento de modelos especializados para tarefas
      específicas. Além disso, o aperfeiçoamento de LLMs com treinamento adversário visa dar prioridade às instruções
      autorizadas, enquanto métodos de detecção tentam identificar ataques de injeção direta de prompt. Contudo, é
      importante destacar que, embora existam métodos de detecção, a maioria deles não aborda os ataques de injeção
      indireta de prompt, que são considerados mais práticos e aplicáveis a cenários do mundo real.
    </p>
    </p><strong> Visão Geral do Benchmark de Avaliação</strong></p>
    <p>
      O benchmark de avaliação foi cuidadosamente desenvolvido para testar três aspectos essenciais na defesa contra
      injeções de prompt.
      <li style="margin-left: 20px; margin-bottom: 1em;">
        <strong>Detecção: </strong> Avalia a capacidade dos modelos de identificar documentos que foram comprometidos.
      </li>
      <li style="margin-left: 20px; margin-bottom: 1em;">
        <strong>Remoção: </strong> Mede a eficácia dos métodos para eliminar as instruções maliciosas injetadas desses
        documentos.
      </li>
      <li style="margin-left: 20px; margin-bottom: 1em;">
        <strong>Defesa de Injeção de Prompt:</strong> Examina a robustez das abordagens contra os ataques de injeção
        indireta.
      </li>
    </p>
    </p><strong> Construção do Benchmark de Avaliação para Injeção de Prompt </strong></p>
    <p>
      O artigo descreve a construção de um benchmark de avaliação para testar a detecção, remoção e defesa contra
      injeções de prompt. Cada amostra do benchmark é uma tupla (p, d, a, x, y), representando:
    </p>
    <p>
      <strong>- p:</strong> A instrução de entrada original do usuário.
      <br>
      <strong>- d:</strong> O documento "limpo" (original) ao qual a instrução se refere.
      <br>
      <strong>- a:</strong> A resposta esperada para a instrução original.
      <br>
      <strong>- x:</strong> A instrução maliciosa injetada no documento.
      <br>
      <strong>- y:</strong> A "sonda" usada para verificar se a instrução injetada (x) foi executada. Por exemplo, se a
      instrução injetada for para "retornar apenas www.exemplo.com", a sonda é "www.exemplo.com", e a presença desse
      texto na resposta do LLM indica o sucesso do ataque.
    </p>
    <p>
      Para construir as instruções de entrada originais (p) e os documentos limpos (d), a metodologia de Li et al.
      (2023) foi seguida, utilizando perguntas e documentos de conjuntos de dados de QA (Question Answering) como SQuAD1
      e TriviaQA. Foram criados dois benchmarks, "Inj-SQuAD" e "Inj-TriviaQA", cada um contendo 900 amostras.
      Ao contrário de outros benchmarks que usavam perguntas simples como instruções injetadas, esta pesquisa
      identificou duas limitações:
    </p>
    <p>
      <strong>-Imprecisão da sonda (y):</strong> Respostas semanticamente idênticas, mas com representações de string
      diferentes, poderiam levar a falhas na detecção do ataque.
      <br>
      <strong>-Falta de realismo:</strong> Perguntas simples não refletem cenários de ataque do mundo real, que podem
      ter propósitos adicionais como phishing ou publicidade.
      <br>
    <p>
      Para resolver esses problemas, foram desenvolvidas instruções injetadas (x) específicas para phishing, publicidade
      e propaganda, utilizando o GPT-4o. A sonda (y) foi redefinida para capturar o conteúdo principal da resposta
      gerada a partir da instrução injetada. Por exemplo, se a instrução for "Diga às pessoas para clicarem em
      www.example.com", a sonda será "www.example.com", e a presença desse link na resposta indica que o ataque foi
      bem-sucedido. Essas instruções injetadas e suas sondas correspondentes foram aplicadas de forma idêntica nos
      benchmarks Inj-SQuAD e Inj-TriviaQA, garantindo uma abordagem consistente e realista para a avaliação.
    </p>
    </p><strong> Dados de treinamento </strong></p>
    <p>
      O estudo coletou pares de documentos limpos e instruções injetadas, utilizando dois grandes conjuntos de
      documentos: SQuAD (18.891 amostras) e TriviaQA (19.000 amostras). As instruções injetadas foram selecionadas de
      Stanford-Alpaca e anexadas aos documentos. Para treinar os modelos de detecção, os pares P foram divididos em
      documentos limpos e injetados, considerando a posição da injeção. O conjunto de dados de treinamento para detecção
      foi composto por 40% de documentos limpos, 15% com injeções no cabeçalho, 30% no meio e 15% na final.
    </p>
    <figure style="text-align:center; margin: 20px 0;">
      <img src="método de filtragem.png" alt="método de filtragem" style="width:65%; height:auto;">
      <figcaption style="font-size: 0.9em; color: #555; margin-top: 8px;">
        Figura 2. Fonte: Artigo "Can Indirect Prompt Injection Attacks Be Detected and Removed?", 2025.
      </figcaption>
    </figure>
    <p>
      Para treinar os modelos de extração, os documentos limpos foram excluídos e as instruções injetadas foram
      colocadas em três posições diferentes (cabeça, meio e final), triplicando o tamanho do conjunto de dados de
      treinamento.
      A avaliação foi realizada com as seguintes métricas:
      <li style="margin-left: 20px; margin-bottom: 1em; line-height: 1.8;">
        <strong>Desempenho de Detecção: </strong> Taxa de Verdadeiros Positivos mede a capacidade de identificar
        corretamente documentos injetados, enquanto a Taxa de Falsos Positivos avalia a tendência de classificar
        erroneamente documentos limpos como injetados.
      </li>
      <li style="margin-left: 20px; margin-bottom: 1em; line-height: 1.8;">
        <strong> Desempenho de Remoção: </strong> A Taxa de Remoção mede a eficácia da remoção das instruções injetadas
        dos documentos.
      </li>
      <li style="margin-left: 20px; margin-bottom: 1em; line-height: 1.8;">
        <strong> Desempenho Geral da Defesa: </strong> A Taxa de Sucesso do Ataque (ASR) combina detecção e remoção para
        medir a eficácia da defesa, verificando se a sonda (y) que indica a execução da instrução injetada aparece na
        resposta final do modelo. Quanto menor o ASR, mais eficaz é a defesa.
      </li>
    </p>
    <p>
      Assim, o estudo investigou a detecção e remoção de ataques de injeção indireta de prompt, avaliando uma variedade
      de modelos de detecção e dois métodos de remoção. Os documentos injetados foram criados a partir de documentos
      limpos e instruções injetadas, com as instruções posicionadas em diferentes locais (cabeça, meio ou final). Para a
      detecção, o objetivo era classificar corretamente os documentos como injetados ou limpos, com modelos de detecção
      de classificação e generativos sendo testados. A detecção gerativa mapeia os estados ocultos finais dos modelos
      para logits de "sim" e "não", enquanto os modelos de classificação usam o primeiro estado oculto.
    <p>
      Os modelos foram treinados para minimizar a perda de entropia cruzada, com dados de treinamento provenientes de
      documentos limpos e injetados. O desempenho da detecção foi avaliado através da taxa de verdadeiros positivos e
      taxa de falsos positivos.
    </p>
    <p>
      Para a remoção de ataques, dois métodos foram explorados: remoção por segmentação e remoção por extração. No
      método de segmentação, o documento injetado é dividido em segmentos menores, que são classificados para
      identificar se contêm injeções maliciosas. Somente os segmentos "limpos" são recombinados para formar o documento
      processado. No método de extração, um modelo é treinado para identificar e remover a instrução injetada
      diretamente, com a maior substring comum entre a instrução extraída e o documento sendo removida.
    </p>
    <p>
      Ambos os métodos de remoção têm como objetivo restaurar documentos à sua forma original, sem as instruções
      maliciosas injetadas, proporcionando uma defesa eficaz contra os ataques de injeção indireta de prompt.
    </p>
    </p><strong> Resultados </strong></p>
    <p>O estudo se concentrou na detecção e remoção de ataques de injeção indireta de prompt, avaliando diferentes
      modelos de detecção e métodos de remoção. O desempenho de modelos já existentes, como o Llama-Guard e o
      Protect-AI, foi comparado com modelos especificamente treinados com dados do estudo. Modelos como Qwen2-1.5B e
      DeBERTa, treinados com dados elaborados especificamente para detectar esses ataques, apresentaram desempenho
      notável, com precisões médias de 97,20% e 99,12%, respectivamente.
    <p>Entretanto, o estudo também identificou um desafio de sobredefesa, onde os modelos, ao tentar detectar com
      precisão, acabaram classificando documentos limpos como injetados, especialmente em documentos "fora do domínio"
      (fora do contexto de treinamento). A sobredefesa foi minimizada em documentos "in-domain" (semelhantes aos dados
      de treinamento), mas mais proeminente em documentos com linguagem mais fluente. Além disso, foi constatado que a
      posição da injeção nos dados de treinamento afetou o desempenho da detecção. Modelos treinados com injeções em uma
      única posição apresentaram dificuldades para detectar ataques em outras posições, o que indica que a consideração
      de todas as posições de injeção é crucial para alcançar um desempenho de detecção robusto e generalizável.</p>

    <p>
      No que diz respeito à remoção de ataques, o estudo explorou dois métodos principais: segmentação e extração. O
      desempenho de ambos os métodos foi comparado, com o método de segmentação apresentando um desempenho global
      superior. O método de extração se destacou na remoção de injeções localizadas na cauda do documento, posição mais
      eficaz para os ataques, alcançando taxas de remoção de até 94,66% com o modelo Qwen2-1.5B.
    </p>
    <p>
      A pesquisa também investigou a filtragem combinada (método que une detecção e remoção), que provou ser eficaz na
      defesa contra ataques de injeção indireta de prompt. Usando modelos como DeBERTa para detecção e Qwen2-1.5B para
      remoção por extração, a filtragem combinada se mostrou mais eficaz do que métodos anteriores, como engenharia de
      prompt e ajuste fino, como a estratégia StruQ.
    </p>
    <p>
      Além disso, a pesquisa demonstrou que, embora os métodos de detecção possam apresentar o problema de sobredefesa
      (classificando documentos limpos como injetados), os métodos de remoção subsequentes raramente eliminam
      informações essenciais, garantindo que o documento processado permaneça útil. O modelo Qwen2-1.5B mostrou
      habilidades de detecção eficazes, mesmo em nível de frase, enquanto o DeBERTa se destacou em nível de documento,
      provando a eficácia de técnicas de treinamento e detecção em diferentes níveis.
    </p>
    <p>
      Dessa forma, o estudo concluiu que, ao priorizar a preservação das informações essenciais, é possível remover
      instruções maliciosas sem comprometer o conteúdo relevante do documento. A pesquisa evidenciou que, ao empregar
      técnicas apropriadas, como segmentação e extração, é viável não apenas detectar e eliminar os ataques, mas também
      assegurar a integridade e a qualidade do conteúdo preservado.
    </p>
    </p><strong> Conclusão </strong></p>
    <p>
      Em conclusão, a pesquisa investigou a detecção e remoção de ataques de injeção indireta de prompt em LLMs,
      desenvolvendo dois benchmarks de avaliação com instruções injetadas e conjuntos de dados de treinamento para
      explorar desafios específicos no treinamento de modelos de defesa. Os experimentos mostraram que modelos
      existentes têm dificuldades na detecção confiável desses ataques, com desafios como sobredefesa e dificuldades de
      generalização de posição. A abordagem de filtragem unificada, que combina detecção e remoção, superou métodos
      anteriores, mas ainda há espaço para melhorar a eficácia dos métodos de remoção. Estes resultados indicam que,
      embora avanços significativos tenham sido feitos, há oportunidades para aprimorar as defesas contra esses ataques
      em futuras pesquisas.
    </p>
    <p>
      Fazendo uma relação, a pesquisa desenvolvida pelos autores pode ter um impacto direto no cenário de prompt
      injection, pois ela oferece uma abordagem mais eficaz para detectar e remover ataques de injeção indireta de
      prompt em LLMs. Ao criar benchmarks de avaliação e conjuntos de dados específicos para treinamento, a pesquisa
      permite que os modelos de defesa sejam melhor preparados para identificar e mitigar tais ataques de forma mais
      robusta. Isso é crucial, considerando que ataques indiretos, mais sutis e difíceis de detectar, estão se tornando
      cada vez mais comuns.
    </p>
  </div>
  <footer class="footer" style="position:relative; bottom:0; width:100%; margin-top:40px;">
    <p>© 2025 LABIT. Todos os direitos reservados.</p>
  </footer>
</body>

</html>