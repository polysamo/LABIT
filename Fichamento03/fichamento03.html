<!DOCTYPE html>
<html lang="pt-br">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>LABIT</title>
  <link rel="stylesheet" href="style.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css">

</head>
<body>

  <!-- NAVBAR -->
  <nav class="navbar">
    <div class="nav-logo">
      <img src="labit.jpg" alt="LABIT">
    </div>
    <ul class="nav-links">
      <li><i class="bi bi-house-door"></i> INÍCIO</li>
      <li><i class="bi bi-people"></i> SOBRE NÓS</li>
      <li><i class="bi bi-journal-text"></i> ARTIGOS</li>
      <li><i class="bi bi-search"></i> PROJETOS </li>
      <li><i class="bi bi-newspaper"></i> NOTÍCIAS</li>
      <li><i class="bi bi-envelope"></i> CONTATO</li>
    </ul>
    <div class="nav-search">
    <i class="bi bi-search"></i>
    <input type="text" placeholder="Buscar...">
  </div>
  </nav>

  <!-- CARD COM CONTEÚDO -->
  <div class="card">
    <section class="breadcrumb">
      ARTIGOS / SEGURANÇA DA INFORMAÇÃO / PROMPT INJECTION
    </section>

    <h2>Fichamento do artigo: “InjectBench: An Indirect Prompt Injection Benchmarking Framework”</h2>

    <div class="meta">
    <span><i class="bi bi-calendar-event"></i> 05 de junho de 2025</span>
    <span><i class="bi bi-at"></i>labit.ufpa</span>
  </div>

  <hr class="divider">

    <p>
      O artigo <strong>“InjectBench: An Indirect Prompt Injection Benchmarking Framework”</strong> foi desenvolvido e escrito pelo pesquisador Nicholas Ka-Shing Kong, além de ter sido publicado na <i>Virginia Tech</i> em 2024 e disponibilizado de forma online.  
    </p>
    <p>
      Os Grandes Modelos de Linguagem (LLMs) como ChatGPT e Llama, apesar de sua capacidade em tarefas textuais, enfrentam uma limitação própria: seu conhecimento é restrito à "data de corte" de seus dados de treinamento, impedindo respostas atualizadas e podendo gerar alucinações. Para contornar a limitação de dados dos LLMs, surgem as aplicações aumentadas de LLM, conhecidas como Plugins. Eles permitem o acesso a informações externas e servioços. No entanto, essa integração introduz um grave risco: a injeção indireta de prompt. Nela, atacantes podem inserir comandos maliciosos em dados externos, explorando a incapacidade do LLM de diferenciar instruções de conteúdo. 
    </p>
    <p>
      A pesquisa sobre injeções indiretas de prompt em LLMs enfrenta dois grandes desafios: a escassez de benchmarks robustos para sua análise qualitativa, já que os estudos focam mais em ataques diretos ou demonstrações; e a ineficácia dos classificadores atuais, que falham em detectar consequências como manipulação de conteúdo ou fraude por priorizarem apenas toxicidade. Para preencher essas falhas, o artigo propõe uma estrutura automatizada. Ela permite gerar amostras de ataque que induzem <i>jailbreak</i> em cenários indiretos, facilitando o desenvolvimento e a medição de defesas. A estrutura é generalizável e foi usada para criar um dataset público de 1670 amostras, que serve como base para o estudo de ataques e defesas em diversos LLMs.
    </p>
    <p>
      O InjectBench oferece um conjunto de dados público para benchmarking de ataques em tarefas de sumarização, cobrindo vetores como manipulação de conteúdo, indisponibilidade e fraude/malware. Além de ser uma ferramenta para avaliação, ele funciona como uma estrutura flexível de ataque sintético, permitindo a criação e personalização de amostras de ataques. Utilizando estratégias de avaliação baseadas em LLMs, validadas por humanos, o InjectBench mede a qualidade das instruções maliciosas e o sucesso dos ataques. O estudo também realiza uma análise detalhada de ataques em quatro famílias de LLMs, identificando suas vulnerabilidades, e investiga defesas preliminares. Os resultados indicam que é possível identificar ameaças com boa precisão e reduzir significativamente o sucesso dos ataques usando técnicas de detecção e amostragem aleatória, sem comprometer a utilidade do modelo.
    </p>
    <P>Na imagem abaixo é demostrado um exemplo de ataque  de  injeção  indireta  de  prompt  em  uma  tarefa  de  sumarização.<P>
    <figure style="text-align:center; margin: 20px 0;">
      <img src="indirect injection.png" alt="indirect injection" style="width:65%; height:auto;">
      <figcaption style="font-size: 0.9em; color: #555; margin-top: 8px;">
        Figura 1. Fonte: Artigo "InjectBench: An Indirect Prompt Injection Benchmarking Framework", 2024.
      </figcaption>
    </figure>
    <p>
      O modelo de ameaça para injeções indiretas de prompt aborda um cenário no qual um plugin confiável interage com recursos de terceiros não confiáveis. Esse cenário é focado na sumarização de conteúdo online, onde um invasor insere instruções maliciosas em um site que será processado pelo LLM (via plugin), resultando em uma resposta falsa ou manipulada para o usuário. Os objetivos do atacante envolvem a disseminação de desinformação por meio de três tipos de ataques:
      
        <li style="margin-left: 20px; margin-bottom: 1em;">
            Manipulação de conteúdo, alterando sutilmente a narrativa do resumo;
        </li>
        <li style="margin-left: 20px; margin-bottom: 1em;">
            Indisponibilidade, bloqueando falsamente o acesso à informação original;
        </li>
        <li style="margin-left: 20px; margin-bottom: 1em;">
            Fraude/malware, coagindo o usuário a clicar em links maliciosos tematicamente relacionados.
        </li>
    </p>
    <p>
        O invasor tem a capacidade de injetar instruções em sites externos, mas não pode alterar o plugin, assim o foco é na manipulação da resposta do plugin ao usuário final.
    </p>
    <p>
        O artigo faz uma revisão de literatura sobre injeções indiretas de prompt em LLMs, destacando tipos de ataques e defesas existentes. Os ataques incluem Jailbreaking, que contorna as medidas de segurança dos LLMs para gerar conteúdo prejudicial, Injeção de Prompt Padrão, que substitui a instrução original do LLM, e Vazamento de Prompt, que busca revelar informações sensíveis sobre o sistema. A defesa contra esses ataques é abordada em duas categorias principais: soluções integradas, como o Aprendizado por Reforço com Feedback Humano (RLHF), que ajusta os pesos do modelo para maior segurança, e soluções externas, que incluem defesas de detecção (identificação de entradas maliciosas) e resilientes (mitigação dos efeitos dos ataques).
    </p>
    <p>
        A avaliação da eficácia de ataques de injeção indireta é desafiadora devido à natureza imprevisível de seus efeitos. Para isso, benchmarks como Open Prompt Injection, InjecAgent e BIPIA são utilizados, embora o artigo se distinga ao não depender da curadoria manual de instruções maliciosas. Em vez disso, ele propõe uma estrutura que permite aos próprios LLMs gerar ataques criativos, com a avaliação feita por meio de um simples prompt e ajustes de precisão. Isso elimina a necessidade de benchmarks tradicionais de PNL e amplia o escopo da avaliação.
    </p>
    </p><strong> InjectBench </strong></p>
    <figure style="text-align:center; margin: 20px 0;">
      <img src="injectBench.png" alt="injectBench" style="width:65%; height:auto;">
      <figcaption style="font-size: 0.9em; color: #555; margin-top: 8px;">
        Figura 2. Fonte: Artigo "InjectBench: An Indirect Prompt Injection Benchmarking Framework", 2024.
      </figcaption>
    </figure> 
    <p>
        O artigo propõe o InjectBench, uma estrutura que permite à comunidade criar e avaliar ataques de injeção indireta de prompt em qualquer conjunto de dados textuais. A estrutura é composta por três componentes principais: a criação de amostras de ataque, uma camada de defesa modular e um método de avaliação.
        <li style="margin-left: 20px; margin-bottom: 1em; line-height: 1.8;">
            <strong>Componente Benigno: </strong> Simula um texto padrão de site, tornando o ataque mais difícil de detectar ao parecer uma entrada normal no fluxo de dados do LLM;
        </li>
        <li style="margin-left: 20px; margin-bottom: 1em; line-height: 1.8;">
            <strong>Componente Separador:</strong> Cria uma quebra de contexto entre os prompts do sistema e a instrução maliciosa, facilitando a interpretação da instrução como um comando direto.
        </li>
        <li style="margin-left: 20px; margin-bottom: 1em; line-height: 1.8;">
            <strong>Instrução Maliciosa:</strong> Busca manipular o conteúdo, afetar a disponibilidade ou fornecer links fraudulentos. Se dada diretamente ao LLM, deveria ser rejeitada.
        </li>
    </p>
    <p><strong>Metodologia</strong></p>
    <p>
        A metodologia utilizada pelo artigo se divide em três partes que serão explicadas a seguir:
        <li style="margin-left: 20px; margin-bottom: 1em; line-height: 1.8;">
            <strong>Componente Benigno</strong> 
            <br>
            O componente benigno de cada amostra representa o conteúdo normal do site. A pesquisa utilizou conjuntos de dados específicos com informações sensíveis, mais propensos a serem alvos de injeções indiretas de prompt. Esses conjuntos incluem:
            <br>
                <strong>- CC-News:</strong> Artigos de notícias em inglês.
            <br>
                <strong>- FAQIR:</strong> Pares de perguntas e respostas do Yahoo! Answers.
            <br>
                <strong>- WikiHow:</strong> Artigos da base de conhecimento online.
            <br>
                <strong>- {R}ecipe{NLG}:</strong> Conjunto de dados de receitas.
            <br>
                <strong>- Amazon Review Data:</strong> Avaliações de produtos de beleza.
            <br>
            Esses conjuntos foram escolhidos por sua qualidade e confiabilidade, garantindo que o conteúdo seja seguro e livre de instruções maliciosas pré-existentes. A metodologia do InjectBench seria, então, adaptável a qualquer conjunto de dados da web.
        </li>
        <li style="margin-left: 20px; margin-bottom: 1em; line-height: 1.8;">
            <strong>Componente Separador</strong> 
        <br>
            O componente separador é muito importante para fazer o LLM diferenciar claramente o contexto do conteúdo benigno da instrução maliciosa, aumentando a eficácia do ataque. Enquanto trabalhos anteriores usam instruções ou delimitadores de caracteres especiais, esta pesquisa explora um separador específico de tarefa. A ideia é que, uma vez que o plugin conclua a tarefa inicial (como sumarizar um texto), ele seguirá a instrução subsequente, que é a instrução do atacante. Para o cenário de sumarização, o resumo do texto benigno é usado literalmente como separador, denominado Separador de Resumo.
    <figure style="text-align:center; margin: 20px 0;">
      <img src="tipos de sepadores.png" alt="tipos de separadores" style="width:70%; height:auto;">
      <figcaption style="font-size: 0.9em; color: #555; margin-top: 8px;">
        Figura 4. Fonte: Artigo "InjectBench: An Indirect Prompt Injection Benchmarking Framework", 2024.
      </figcaption>
    </figure>
        </li>
        <li style="margin-left: 20px; margin-bottom: 1em; line-height: 1.8;">
            <strong>Geração e Seleção de Instruções Maliciosas</strong> 
            <br>
            Uma abordagem eficaz para ataques indiretos de injeção rápida envolve incorporar instruções de malware diretamente no conteúdo de um site, mantendo uma conexão temática com o texto. O InjectBench utiliza essa técnica, aproveitando a capacidade de raciocínio dos LLMs para gerar instruções maliciosas contextualizadas. O Vicuna 13B V1.1 foi escolhido devido à sua capacidade de gerar instruções maliciosas consistentes e comparáveis ao ChatGPT, apesar de ser um modelo de tamanho menor.
            Para contornar as medidas de segurança existentes ao gerar essas instruções, foi utilizado o método Evil Confidant Jailbreak. A seguir, o modelo é consultado com prompts específicos, aplicados a amostras de dados benignos para gerar cinco candidatas a instruções maliciosas. Esta abordagem melhora tanto a qualidade linguística das instruções quanto facilita a análise de expressões regulares. O conteúdo do site é dinâmicamente inserido no prompt, substituindo o espaço reservado «COMPONENTE BENIGNO».
            Por fim, o Mistral 7B Instruct V0.2 é usado para selecionar a melhor instrução, atribuindo uma pontuação de 0 a 10 para cada critério de qualidade. As instruções com uma soma ponderada inferior a 5 são filtradas, garantindo que apenas as mais eficazes sejam escolhidas para os ataques.
        </li>
    <figure style="text-align:center; margin: 20px 0;">
      <img src="exemplos de prompt.png" alt="exemplos de prompt" style="width:65%; height:auto;">
      <figcaption style="font-size: 0.9em; color: #555; margin-top: 8px;">
        Figura 5. Fonte: Artigo "InjectBench: An Indirect Prompt Injection Benchmarking Framework", 2024.
      </figcaption>
    </figure>    
    <p><strong>Validações</strong></p>
    <p> 
        As validações são essenciais para garantir que o InjectBench produza amostras de alta qualidade e eficazes para testar ataques de injeção indireta de prompt em LLMs e que os componentes benignos e separadores não contenham instruções maliciosas, sendo representativos dos dados reais da web. No entanto, a validação das instruções maliciosas exige uma análise mais rigorosa. Após gerar 120 instruções, essas foram avaliadas e categorizadas como válidas ou inválidas, com um filtro de pontuação abaixo de 5, que resultou em melhores instruções.
    </p>
    <p>
        Para a avaliação da qualidade, geraram-se 700 amostras para três categorias de ataque: disponibilidade, fraude/malware e conteúdo manipulado. Após filtragem, 15% a 26% das amostras foram descartadas, mas as restantes foram consideradas de alta qualidade. A avaliação foi realizada usando um modelo de LLM ajustado, com um método de classificação de "sim" ou "não", e a precisão foi de 0,799 ao comparar a avaliação automatizada com a humana.
        Em comparação com outros estudos, o InjectBench apresentou uma boa precisão, com 1,000 de precisão entre o avaliador humano e a avaliação automatizada, mostrando que a métrica é eficaz para a avaliação em larga escala. A análise também revelou que a precisão do modelo de avaliação pode ser ajustada para preferências pessoais, garantindo uma avaliação confiável e escalável.
    </p>
    <figure style="text-align:center; margin: 20px 0;">
      <img src="exemplo de validação.png" alt="exemplo de validação" style="width:65%; height:auto;">
      <figcaption style="font-size: 0.9em; color: #555; margin-top: 8px;">
        Figura 6. Fonte: Artigo "InjectBench: An Indirect Prompt Injection Benchmarking Framework", 2024.
      </figcaption>
    </figure>    
    <p><strong>Validação dos LLMs no InjectBench</strong></p>
    <p>
        A pesquisa vai analisar o desempenho dos LLMs em ataques de injeção indireta de prompt, incluindo o impacto do componente separador e as modificações nas instruções maliciosas.
        <br>
        <strong>Configurações do Experimento: </strong> Foram selecionadas 300 amostras de ataque, com 100 amostras para cada uma das três categorias: manipulação de conteúdo, disponibilidade e fraude/malware. Quatro LLMs foram avaliados: Mistral 7B, Llama-2 Chat 13B, Vicuna 33B v1.3 e Llama-1 7B. A avaliação do sucesso dos ataques foi feita com a métrica Taxa de Sucesso do Ataque (ASR).
        <br>
        <strong>Resultados de Sucesso dos Ataques:</strong> Ataques de disponibilidade e fraude/malware tiveram um ASR maior, sendo mais bem-sucedidos que os de manipulação de conteúdo. Modelos maiores, como Vicuna 33B e Llama-2, apresentaram um ASR mais alto, embora o Llama-2 tenha sido o mais suscetível a ataques, o que pode ser atribuído à sua capacidade de seguir instruções com mais precisão.
        <br>
        <strong>Impacto do Componente Separador:</strong> A eficácia dos separadores foi testada com 300 amostras. Separadores de foco, que mudam explicitamente a atenção do plugin, mostraram o melhor desempenho, com um ASR médio de 52,3%. No entanto, esses separadores foram mais facilmente detectados por sistemas de defesa como o LLMGuard (97% de taxa de detecção). Já separadores de resumo, mais sutis, tiveram uma taxa de detecção de 36%, mas ainda assim mantiveram um ASR relativamente alto (46,9%).
        <br>
        <strong>Impacto das Modificações nas Instruções Maliciosas:</strong> A introdução de modificações nas instruções maliciosas diminuiu o ASR, especialmente em modelos maiores como Vicuna e Llama-2. Isso ocorre porque esses modelos são mais habilidosos em lidar com instruções complexas. Modelos menores como Mistral e Alpaca Llama apresentaram ASR mais baixo após as modificações, indicando que modelos maiores são mais suscetíveis a estratégias de injeção mais refinadas.
        <br>
        Em resumo, separadores de foco são eficazes para ataques, mas podem ser facilmente detectados, enquanto separadores de resumo oferecem uma abordagem mais furtiva, mantendo um bom ASR. As modificações nas instruções maliciosas tiveram menos impacto em modelos maiores, mas reduziram o sucesso dos ataques.
    </p>
    <p><strong>Defesas</strong></p>
    <p>
        Foram propostas dois tipos de defesas contra injeções indiretas de prompt: defesas de detecção e defesas resilientes. No qual, a defesa de detecção busca aprimorar as estratégias existentes, permitindo que o LLM identifique e repita instruções maliciosas embutidas no texto, aproveitando sua capacidade de raciocínio. No entanto, modelos menores podem parafrasear as instruções, e uma similaridade de cosseno é utilizada para garantir que a instrução encontrada seja suficientemente próxima da original. Essa estratégia precisa de mais treinamento, especialmente para modelos menores.
        <br>
        Para defesas resilientes, foi proposta uma abordagem baseada em amostragem aleatória de frases e geração de resumos, mantendo o tema central consistente e minimizando a influência de instruções maliciosas. Dois métodos foram testados: sumarização LLM, que usa o próprio LLM para resumir, e agregação de incorporação semântica (SEA), que codifica os resumos e retorna a codificação mais próxima da média. A amostragem aleatória foi considerada mais forte que a marcação de dados, proporcionando uma defesa eficaz, mas com alguma perda de informação, o que impacta a qualidade dos resumos.
    </p>
    <figure style="text-align:center; margin: 20px 0;">
      <img src="prompt de defesa.png" alt="exemplo de validação de defesa" style="width:65%; height:auto;">
      <figcaption style="font-size: 0.9em; color: #555; margin-top: 8px;">
        Figura 7. Fonte: Artigo "InjectBench: An Indirect Prompt Injection Benchmarking Framework", 2024.
      </figcaption>
    </figure> 
    <p><strong>Avaliação das Defesas no InjectBench</strong></p>
    <p>
        Nos testes de defesas de detecção, a Defesa Proativa e a LLMGuard mostraram limitações. A Defesa Proativa teve bom desempenho na identificação de instruções maliciosas, mas gerou falsos positivos, enquanto a LLMGuard foi ineficaz contra as amostras de ataque. Em contraste, a Defesa do Localizador de Instruções obteve uma precisão de 78%, superando métodos anteriores. No entanto, essa abordagem tende a parafrasear as instruções, o que limita sua eficácia em modelos menores.
        <br>
        As defesas resilientes foram avaliadas com base na Taxa de Sucesso do Ataque (ASR) e na qualidade dos resumos usando a métrica ROUGE-1. A marcação de dados mostrou boa eficácia, mas não teve impacto no Mistral 7B. Já a amostragem aleatória reduziu o ASR significativamente, com uma boa relação custo-benefício entre defesa e qualidade do resumo, especialmente no Mistral e Vicuna 33B.
        <br>
        Em resumo, a amostragem aleatória se mostrou uma defesa promissora, embora com perda de dados, enquanto a marcação de dados preservou melhor a utilidade do modelo. Ambas as defesas apresentaram bom desempenho em reduzir os ataques, com a amostragem aleatória oferecendo uma defesa mais forte a custo de menor precisão nos resumos.
    </p>
    <p><strong>Conclusão</strong></p>
    <p>
        Fazendo uma relação, a pesquisa desenvolvida pelo autor pode ter um impacto direto no cenário de prompt injection, oferecendo uma estrutura robusta e automatizada para criar e avaliar ataques. O InjectBench permite não apenas simular ataques de injeção indireta, mas também testar a eficácia de defesas de detecção e resilientes, como o Localizador de Instruções baseado em LLM e a sumarização por amostragem aleatória. Com isso, a ferramenta pode acelerar o desenvolvimento de soluções para mitigar riscos de segurança em ambientes que utilizam LLMs, impactando diretamente na criação de sistemas mais seguros e robustos contra ataques maliciosos.
    </p>
  </div>
  <footer class="footer">
    <p>© 2025 LABIT. Todos os direitos reservados.</p>
  </footer>
</body>
</html>
